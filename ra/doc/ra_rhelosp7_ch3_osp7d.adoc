[chapter 3]
== OpenStack Platform 7 director

{ro} delivers an integrated foundation to create, deploy, and
scale a secure and reliable public or private OpenStack cloud.
{ro} starts with the proven foundation of {rhel} and integrates Red
Hat's OpenStack Platform technology to provide a production-ready
cloud platform backed by an ecosystem of more than 350 certified partners.

{ro} 7 is based on the community Kilo OpenStack release. This
release is Red Hat's fifth iteration of {ro}. It has been
successfully deployed by Red Hat customers worldwide across diverse
vertical industries including financial, telecommunications, and
education.

{ro} 7 introduces {ro} director, a cloud installation and
lifecycle management tool chain. Director is the first {ro}
installer to deploy OpenStack on and with OpenStack. This section of
the paper introduces {ro} director's architecture and features:

* Simplified deployment through ready-state provisioning of bare metal resources.
* Flexible network definitions
* High availability via tight integration with the {rhel} Server High
  Availability Add-on
* Integrated setup and installation of Red Hat Ceph Storage 1.3
* Content management via the {cdn} or {rhss}.

=== Ready State Provisioning and Server Roles
{ro} director is a converged installer. It combines mature upstream
OpenStack deployment projects (`TripleO` and `Ironic`) with
components from Red Hat's past {ro} installers.

`TripleO` stands for _OpenStack on OpenStack_. `TripleO` is an upstream
OpenStack project that uses an existing OpenStack environment to install
a production OpenStack environment. The deployment environment is called
the `undercloud`. The production environment is called the `overcloud`.

The `undercloud` is the `TripleO` control plane. It uses native OpenStack APIs
and services to deploy, configure, and manage the production OpenStack
deployment. The `undercloud` defines the overcloud with `Heat` templates
then deploys it via the `Ironic` baremetal provisioning service. {ro} director
includes predefined `Heat` templates for the basic server roles that comprise
the overcloud. Customizable templates allow director to deploy,
redeploy, and scale complex overclouds in a repeatable fashion.

`Ironic` is a community bare-metal provisioning project.
Director uses `Ironic` to deploy the overcloud servers. `Ironic`
gathers information about baremetal servers via a discovery mechanism
known as introspection. `Ironic` pairs servers with bootable
images and installs them via PXE and remote power management.

{ro} director deploys all servers with the same generic image by
injecting `Puppet modules` into the
image to tailor it for specific server roles. It then
applies host-specific customizations via Puppet including network and
storage configurations.

While the `undercloud` is primarily used to deploy OpenStack, the
`overcloud` is a functional cloud available to run virtual machines
and workloads. Servers in the following roles comprise the overcloud:

[[server-roles]]
[glossary]
*Control*::
    This role provides endpoints for REST-based API queries to the
    majority of the OpenStack services. These include Compute, Image,
    Identity, Block, Network, and Data processing.  The controller can
    run as a standalone server or as a HA cluster.
*Compute*::
    This role provides the processing, memory, storage, and
    networking resources to run virtual machine instances. It runs
    the KVM hypervisor by default. New instances are spawned across
    compute nodes in a round-robin fashion based on resource
    availability.
*Block storage*::
    This role provides external block storage for HA
    controller nodes via the OpenStack Block Storage service `Cinder`.
*Ceph storage*::
    Ceph is a distributed object store and file system. This role
    deploys Object Storage Daemon (OSD) nodes for Ceph clusters. It
    also installs the Ceph Monitor service on the controller.
*Object storage*::
    This role provides external Account, Container, and Object
    (ACO) storage for the OpenStack Object Storage service, `Swift`,
    by installing a `Swift` proxy server on the controller nodes.

NOTE: The overcloud requires at least one controller and one compute
node. It runs independently from the undercloud once it is
installed. This reference architecture uses the Control, Compute, and Ceph
storage roles.

{ro} director also includes *advanced hardware configuration* tools
from the eNovance SpinalStack installer. These tools validate server
hardware prior to installation. *Profile matching* lets administrators
specify hardware requirements for each server role. {ro} director only
matches servers that meet minimum hardware requirements for each role.
Profile to matching is performed after introspection but prior to deployment.

{ro} director also supports pre-installation *benchmark collection*.
Servers boot to a customized RAMdisk and run a series of benchmarks.
The benchmarks report performance outliers prior to installation.

NOTE: {ro} 7 requires {rhel} 7 Server on all servers.
Supported guest operating systems can be found at
https://access.redhat.com/articles/973163. Deployment limitations are
listed at https://access.redhat.com/articles/1436373.

=== Network Isolation
OpenStack requires multiple network functions. While it is possible to
collapse all network functions onto a single network interface,
isolating communication streams in their own physical or virtual
networks provides better performance and scalability.

{ro} director supports isolating network traffic by type. One or more
network traffic types can be assigned to a physical,
virtual, or bonded interface. Multiple traffic types can be combined
across the same physical interfaces or switches. Each OpenStack
service is bound to an IP on a particular network. In a cluster a
service virtual IP is shared among all of the HA controllers.

{ro} director supports network isolation for the following traffic
types:

[[traffic-types]]
[glossary]
*Provisioning*::
    The control plane installs the overcloud via this network. All cluster
    nodes must have a physical interface attached to the provisioning network.
    This network carries DHCP/PXE and TFTP traffic so it must be
    provided on a dedicated interface or a native VLAN to the boot interface. The
    provisioning interface can act as a default gateway for
    the overcloud if there is no other gateway on the network.

NOTE: {ro} director 7.1 supports static IPs on the provisioning network.
    Using static IPs requires additional parameters in the network
    isolation Heat templates for setting static IPs, routes, and DNS
    servers. <<configure-network-isolation-section>> describes the
    additional parameters for configuring static IPs on the
    provisioning network.

NOTE: Disable PXE on the remaining interfaces to ensure the servers boot from this network.

*External*::
    This network provides overcloud nodes with external connectivity.
    Controller nodes connect the external network to an `Open vSwitch`
    bridge and forward traffic originating from hypervisor instances
    through it. The `Horizon` service and OpenStack public API endpoints
    can share this network or they can be broken out to an optional
    public API network.
*Internal API*::
    This network exposes internal OpenStack API endpoints for the
    overcloud nodes. It handles inter-service communication between
    both core OpenStack services and the supporting services. By
    default this network also hosts cluster management traffic used by
    HA services to share data and track cluster state for automated
    failover. It is common practice to break the cluster management
    traffic out to a separate network if it affects performance or
    scaling. Supporting service traffic from the state
    database, message bus, and hostname resolution is also delivered
    via this network.
*Tenant*::
    Virtual machines communicate over the tenant network. It supports
    three modes of operation: VXLAN, GRE, and VLAN. VXLAN and GRE
    tenant traffic is delivered via software tunnels on a single VLAN.
    In VLAN mode, individual VLANs correspond to tenant networks.
*Storage*::
    This network carries storage communication including `Ceph`, `Cinder`,
    and `Swift` traffic. The virtual machine instances communicate
    with the storage servers via this network. Data-intensive
    OpenStack deployments should isolate storage traffic on a
    dedicated high bandwidth interface, i.e. 10 GB interface. The
    `Glance` API, `Swift` proxy, and `Ceph Public interface` services are
    all delivered via this network.
*Storage Management*::
    Storage management communication can generate large amounts of
    network traffic. This network is shared between the front and back
    end storage nodes. Storage controllers use this network to access
    data storage nodes. This network is also used for storage clustering
    and replication traffic.

Network traffic types are assigned to network interfaces through
`Heat` template customizations prior to deploying the overcloud. {ro}
director supports several network interface types including physical
interfaces, bonded interfaces, and either tagged or native 802.1Q VLANs.

NOTE: Disable DHCP on unused interfaces to avoid unwanted routes and network
loops.

==== Network Types by Server Role
The previous section discussed <<server-roles, server roles>>. Each
server role requires access to specific types of network traffic.
Figure 3.1 <<network-topology-diagram>> depicts the network roles by server type in
this reference architecture.

[[network-topology-diagram]]
.Network topology
image::images/NETWORK.png[caption="Figure 3.1 " title="Network Topology" align="center", scaledwidth="60%"]

The network isolation feature allows {ro} director to segment network
traffic by particular network types. When using network isolation,
each server role must have access to its required network traffic
types.

By default, {ro} director collapses all network traffic to the provisioning
interface. This configuration is suitable for evaluation, proof of
concept, and development environments. It is not recommended for
production environments where scaling and performance are primary
concerns.

Table 1 <<network-topology-table>> summarizes the required network
types by server role.

[[network-topology-table]]
.Network type by server role
[options="header"]
|====
|Role|Network
.2+^.^|Undercloud|External
|Provisioning
.6+^.^|Control|External
|Provisioning
|Storage Mgmt
|Tenant
|Internal API
|Storage
.4+^.^|Compute|Provisioning
|Tenant
|Internal API
|Storage
.3+^.^|Ceph/Block/Object Storage|Provisioning
|Storage Mgmt
|Storage
|====

==== Tenant Network Types
{ro} 7 supports tenant network communication through
the OpenStack Networking (`Neutron`) service. OpenStack Networking supports
overlapping IP address ranges across tenants via the Linux kernel's
`network namespace` capability. It also supports three default
networking types:

[[tenant-network-types]]
[glossary]
*VLAN segmentation mode*::
  Each tenant is assigned a network subnet
  mapped to a 802.1q VLAN on the physical network. This tenant
  networking type requires VLAN-assignment to the appropriate switch
  ports on the physical network.
*GRE overlay mode*::
  This mode isolates tenant traffic in virtual
  tunnels to provide Layer 2 network connectivity between virtual
  machine instances on different hypervisors. GRE does not require
  changes to the network switches and supports more unique network IDs
  than VLAN segmentation. 
*VXLAN*::
  This overlay method similar to GRE. VXLAN combines the ease
  and scalability of GRE with superior performance. This is the default
  mode of operation for {ospver} as of the Y1 release.

Although Red Hat certifies third-party network plug-ins, {ro} director
uses the `ML2` network plugin with the `Open vSwitch` driver by default.

NOTE: {ro} director does not deploy legacy (`Nova`) networking.

=== High Availability
{ro} director's approach to high availability OpenStack leverages Red Hat's
internal expertise with distributed cluster systems. Most of
the technologies discussed in this section are available through the
{rhel} Server High Availability Add On. These technologies are bundled
with {ro} 7 to provide cluster services for production deployments.

==== Cluster Manager and Proxy Server
Two components drive HA for all core and non-core OpenStack
services: the *cluster manager* and the *proxy server*.

The cluster manager is responsible for the startup and recovery of an
inter-related services across a set of physical machines. It tracks
the cluster's internal state across multiple machines. State changes
trigger appropriate responses from the cluster manager to ensure
service availability and data integrity.

===== Cluster manager benefits

. Deterministic recovery of a complex, multi-machine application
  stack.
. State awareness of other cluster machines to co-ordinate service
   startup and failover.
. Shared quorum calculation to determine majority set of surviving
  cluster nodes after a failure.
. Data integrity through fencing. Machines running a non-responsive
   process are isolated to ensure they are not still responding to
   remote requests. Machines are typically fenced via a remotely
   accessible power switch or IPMI controller.
. Automated recovery of failed instances to prevent additional
   load-induced failures.

In {ro}'s HA model, clients do not directly connect to service
endpoints. Connection requests are routed to service endpoints by a
proxy server.

===== Proxy server benefits

. Connections are load balanced across service endpoints
. Service requests can be monitored in a central location
. Cluster nodes can be added or removed without interrupting service

{ro} director uses `HAproxy` and `Pacemaker` to manage HA services and load
balance connection requests. With the exception of `RabbitMQ` and
`Galera`, `HAproxy` distributes connection requests to active nodes in a
round-robin fashion. `Galera` and `RabbitMQ` use persistent options to
ensure requests go only to active and/or synchronized nodes. `Pacemaker`
checks service health at 1 second intervals. Timeout settings vary by
service.

===== Benefits of combining Pacemaker and HAproxy

The combination of `Pacemaker` and `HAproxy`:

* Detects and recovers machine and application failures
* Starts and stops OpenStack services in the correct order
* Responds to cluster failures with appropriate actions including
  resource failover and machine restart and fencing
* Provides a thoroughly tested code base that has been used in
  production clusters across a variety of use cases

The following services deployed by {ro} director do not use the
`HAproxy` server:

. `RabbitMQ`
. `memcached`
. `mongodb`

These services have their own failover and HA mechanisms. In most
cases the clients have full lists of all service endpoints and try
them in a round robin fashion. Individual cluster services are
discussed in the following section.

NOTE: {ro} director uses `Pacemaker` and `HAproxy` for clustering. Red Hat
also supports manually deployed {ro} 7 clustered with `keepalived` and
`HAproxy`. Manual installation is beyond the scope of this document.

==== Cluster models: Segregated versus Collapsed

Cluster services can be deployed across cluster nodes in
different combinations. The two primary approaches are _segregated_ and
_collapsed_.

*Segregated* clusters run each service on dedicated clusters of three
or more nodes. Components are isolated and can be scaled individually.
Each service has its own virtual IP address. Segregating services
offers flexibility in service placement. Multiple services can be run
on the same physical nodes, or, in an extreme case, each service can
run on its own dedicated hardware.

Figure 3.2 <<segregated-cluster>> depicts OpenStack service deployed
in a segregated cluster model. Red Hat supports {ro} 7 services
deployed in a segregated model but it is beyond the scope of this
document.

*Collapsed* clusters run every service and component on the same set of
three or more nodes. Cluster services share the same virtual IP
address set. Collapsed services require fewer physical machines and
are simpler to implement and manage.

Previous {osp} installers deployed segregated
clusters. {ro} director deploys overclouds as collapsed clusters. All
controller nodes run the same services. Service endpoints are bound to
the same set of virtual IP addresses. The undercloud is not clustered.

Figure 3.3 <<collapsed-cluster>> depicts {ro} director's default
approach to deploying collapsed HA OpenStack services.

NOTE: Segregated and collapsed are the dominant approaches to
implementing HA clusters but hybrid approaches are also possible.
Segregate one or more components expected to cause a bottleneck into
individual clusters. Collapse the remainder. Deploying a mixed cluster
is beyond the scope of this document.

[[segregated-cluster]]
.Segregated cluster
image::images/HA_SEGREGATED.png[caption="Figure 3.2: " title="Segregated Cluster Services" align="center", scaledwidth="60%"]

==== Cluster Services and Quorum
Each clustered service operates in one of the following modes:

[[cluster-services-and-quorum]]
[glossary]
*Active/active*::
  Requests are load balanced between multiple
  cluster nodes running the same services. Traffic intended for failed
  nodes is sent to the remaining nodes.
*Active/passive*::
  A redundant copy of a running service is brought
  online when the primary node fails.
*Hot Standby*::
  Connections are only routed to one of several active
  service endpoints. New connections are routed to a standby
  endpoint if the primary service endpoint fails.
*Mixed*::
  Mixed has one of two meanings: services within a group run
  in different modes, or the service runs active/active but is used as
  active/passive. Mixed services are explained individually.
*Single*::
  Each node runs an independent cluster manager that only
  monitors its local service.

A cluster *quorum* is the majority node set when a failure splits the
cluster into two or more partitions. In this situation the majority
fences the minority to ensure both sides are not running the same
services -- a so-called _split brain_ situation. *Fencing* is the
process of isolating a failed machine -- typically via remote power
control or networked switches -- by powering it off. This is necessary
to ensure data integrity.

NOTE: Although `Pacemaker` supports up to 16 cluster nodes, Red Hat
recommends an odd number of cluster members to help ensure quorum during
cluster communication failure. {ro} director requires three
active cluster members to achieve quorum.

==== Cluster Modes for Core Services
This section of the paper describes {ro} director's default cluster mode for each
OpenStack service.

[[collapsed-cluster]]
.Collapsed cluster
image::images/HA_COLLAPSED.png[caption="Figure 3.3: " title="Collapsed Cluter Services" align="center", scaledwidth="60%"]

The following table lists service mode by service.

<<<

[[core-cluster-modes]]
.Core Service Cluster Modes and Description
[options="header"]
|====
|Service|Mode|Description
|*Ceilometer*|Active/active|Measures usage of core OpenStack
components. It is used within `Heat` to trigger application autoscaling.
|*Cinder*|Mixed|Provides persistent block storage to virtual
machines. All services are active/active except `cinder-volume` runs
active/passive to prevent a potential
https://bugzilla.redhat.com/show_bug.cgi?id=1193229[race condition].
|*Glance*|Active/active|Discovers, catalogs, and retrieves virtual
machine images.
|*Horizon*|Active/active|Web management interface runs via `httpd` in
active/active mode.
|*Keystone*|Active/active|Common OpenStack authentication system runs
in `httpd`.
|*Neutron server*|Active/active|`Neutron` allows users to define and join
networks on demand.
|*Neutron agents*|Active/active|Support Layer 2 and 3 communication
plus numerous virtual networking technologies including `ML2` and `Open vSwitch`.
|*Nova*|Active/active|Provides compute capabilities to deploy and run
virtual machine instances.
|*Swift proxy server*|Active/active|Routes data requests to the
appropriate `Swift` ACO server.
|====

==== Cluster Modes for Supporting Services

The majority of the core OpenStack services run in active/active mode.
The same is true for the supporting services, although several of
them field connection requests directly from clients rather than
`HAproxy`.

<<<

The following table lists the cluster mode for the non-core OpenStack
services.

[[supporting-cluster-modes]]
.Supporting Service Cluster Modes and Description
[options="header"]
|====
|Service|Mode|Description
|*Replicated state database*|Active/active|`Galera` replicates databases
to decrease client latency and prevent lost transactions. `Galera` runs
in active/active mode but connections are only sent to one active node
at a time to avoid lock contention.
|*Database cache*|Hot standby|Memory caching system. `HAproxy` does not
manage `memcached` connections because replicated access is still
experimental.
|*Message bus*|Active/active|`AMQP` message bus coordinates job
execution and ensures reliable delivery. Not handled by `HAproxy`.
Clients have a full list of `RabbitMQ` hosts.
|*NoSQL database*|Active/active|NoSQL database `mongodb` supports
`Ceilometer` and `Heat`. Not managed by `HAproxy`. `Ceilometer` servers have a
full list of `mongodb` hosts.
|*Load Balancer*|Active/active|The load balancer `HAproxy` runs in
active/activde mode but is accessed via a set of active/passive
virtual IP addresses.
|====

==== Compute Node and Swift ACO Clustering
{ro} installs compute nodes and `Swift`
storage servers as single-node clusters in order to monitor their
health and that of the services running on them.

In the event that a compute node fails, `Pacemaker` restarts compute
node services in the following order:

1. `neutron-ovs-agent`
2. `ceilometer-compute`
3. `nova-compute`

In the event that a `Swift` ACO node fails, `Pacemaker` restarts `Swift`
services in the following order:

1. `swift-fs`
2. `swift-object`
3. `swift-container`
4. `swift-account`

If a service fails to start, the node where the service is running
will be fenced in order to guarantee data integrity.

NOTE: This article contains more information regarding HA clustering
and {ro}: https://access.redhat.com/articles/1462803[Understanding
RHEL OpenStack Platform High Availability]

=== Ceph Storage Integration
Red Hat Ceph Storage 1.3 is a distributed data object store designed for
performance, reliability, and scalability. {ro} 7 director can deploy
an integrated Ceph cluster in the overcloud. The integrated Ceph
cluster acts as a storage virtualization layer for `Glance` images,
`Cinder` volumes, and `Nova` ephemeral storage. Figure 3.4
<<ceph-integration>> depicts {ro} 7 director Ceph cluster integration
from a high level. The Ceph cluster consists of two types of daemons:
Ceph OSD and Ceph Monitor. The *Ceph OSD Daemon* stores data in pools
striped across one or more disks. Ceph OSDs also replicate, rebalance,
and recover data, and report data usage.

The *Ceph Monitor* maintains a master copy of the Ceph storage map and
the current state of the storage cluster. Ceph clients consult the
Ceph monitor to receive the latest copy of the storage map then
communicate directly with the primary data-owning OSD.

[[ceph-integration]]
.Ceph Integration
image::images/CEPH.png[caption="Figure 3.4: " title="Ceph Integration" align="center", scaledwidth="60%"]

{ro} director can install a Ceph cluster with one or more OSD servers.
By default the OSD server will use free space on its primary disk for
the OSD storage device. Additional OSDs can be configured through
Puppet customization prior to deploying the overcloud. Ceph
performance scales with the number of OSD disks. The Ceph monitor is
installed on the controller nodes whenever a Ceph storage role is
deployed in the overcloud.

This reference architecture includes a 4-node Ceph cluster. Each node
has 10 OSD disks (40 total). The OSDs in the reference architecture
store `Glance` images, host `Cinder` volumes, and provide `Nova`
instances with ephemeral storage.

Consult
https://access.redhat.com/documentation/en/red-hat-ceph-storage/version-1.3/red-hat-ceph-storage-13-red-hat-ceph-architecture/red-hat-ceph-architecture[Ceph
documentation] for more information on Red Hat Ceph Storage 1.3. This https://access.redhat.com/articles/1370143[reference architecture] details how to install and run Ceph with standalone
versions of {osp}.

<<<
